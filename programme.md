---
layout: page
title: Programme
---

Timings and session details are provided below. All times are in local time (i.e., **UTC+1**).

<div class="panel panel-default">
<div class="panel-body">

<div class="card  m-3">

<div class="card-body">

<table style="margin-left: 1em;">
<tbody>
<tr><td>9:00</td><td>- Coffee / Pastries</td></tr>
<tr><td>9:25</td><td>- Welcome </td></tr>
<tr><td>9:35</td><td>- <a href="#keynote">Keynote - Bernd T Meyer</a></td></tr>
<tr><td>10:25</td><td>- <a href="#overview">The Clarity Prediction Challenge Overview + Prizes</a></td></tr>
<tr><td>10:50</td><td>- Coffee/Tea/Pastries</td></tr>
<tr><td>11:10</td><td>- <a href="#poster">Prediction Challenge Poster session</a></td></tr>
<tr><td>12:10</td><td>- <a href="#oral1">Prediction Challenge Oral session</a></td></tr>
<tr><td>13:10</td><td>- Lunch</td></tr>
<tr><td>14:00</td><td>- <a href="#discussion">ISAAR/Clarity Discussion</a></td></tr>
<tr><td></td><td>Future challenges</td></tr>
<tr><td></td><td>Sustainability of future challenge activities</td></tr>
<tr><td>15:00</td><td>- Coffee/Tea  (Posters Cont)</td></tr>
<tr><td>15:20</td><td>- <a href="#oral2">Oral Session / Hearing Aid Speech Enhancement + Other Topics </a> </td></tr>
<tr><td>17:00</td><td>- <a href="#next">Next steps</a></td></tr>
<tr><td></td><td>From spoken speech to sung speech - Cadenza Lyric Challenge</td></tr>
<tr><td></td><td>Conversation in noise  - CHiME-9 ECHI Challenge</td></tr>
<tr><td>17:20</td><td>- Close</td></tr>

</tbody>
</table>
</div>
</div>

<h1 id="keynote">Invited Talk</h1>

<div class="card m-3 mt-4">
  <a name="keynote"></a>

<div class="card-header">
<div class="row align-items-center">

<div class="col-sm-2">
<img src="./assets/images/meyer_photo.png" alt="Speaker" class="float-left rounded-circle" style="width:90%; height:90%;" />
</div>

<div class="col-sm-4">
<h1 class="lead">Bernd T Meyer <div class="text-muted">University of Oldenburg, Germany</div> </h1>
</div>

<div class="col-sm-6">

<h1> Machine learning for computational audiology: Prediction of auditory perception and improvement of speech signals based on deep learning </h1>

<button class="btn btn-primary" style="color:white; margin: 10px; border-radius: 4px;" type="button" data-toggle="collapse" data-target="#collapseAbstractMeyer" aria-expanded="false" aria-controls="collapseAbstractMeyer">
Abstract + Bio
</button>

</div>
</div>
</div>

<!---------------------------------------------------->
<div class="collapse" id="collapseAbstractMeyer">

<div class="card-body">
<h1 class="card-title"> Machine learning for computational audiology: Prediction of auditory perception and improvement of speech signals based on deep learning  </h1>

<h2>Abstract</h2>

To appear.

<h2>Bio</h2>

Bernd T. Meyer received the Ph.D. degree from the University of Oldenburg, Germany, in 2009, where he was a member of the Medical Physics Group. He was a Visiting Researcher in the speech group with the International Computer Science Institute, Berkeley, CA, USA, and worked in the Center for Language and Speech Processing at the Johns Hopkins University, Baltimore, MD, USA. Since 2019, he is professor for Communication Acoustics at the University Oldenburg. His research interests include the relation of speech and hearing, with a special interest in models of human speech perception, automatic speech processing and its applications in hearing technology.

</div>
</div>
</div>
<!---------------------------------------------------->

<h1 id="overview">Clarity Prediction Challenge</h1>

<h2> Overview </h2>

<div class="card  m-3">
<div class="card-body">

<table>
<tbody>

<tr><td valign="top" style="margin-right: 5em; padding: 5px;"></td>
<td valign="top" style="margin-right: 5em; padding: 5px;">10:30-10:50</td><td style="padding: 5px;"><b> The 3rd Clarity Prediction Challenge: A machine learning challenge for hearing aid intelligibility prediction </b> <br /> <span class="author">Jon Barker<sup>1</sup>, Michael A. Akeroyd<sup>2</sup>, Trevor J. Cox<sup>3</sup>, John F. Culling<sup>4</sup>, Jennifer Firth<sup>2</sup>, Simone Graetzer<sup>3</sup> and Graham Naylor<sup>2</sup></span><i> ( <sup>1</sup>University of Sheffield; <sup>2</sup>University of Nottingham; <sup>3</sup>University of Salford; <sup>4</sup>Cardiff University)</i> </td>
</tr>

</tbody>
</table>

</div>
</div>

<h2 id="poster"> Poster Session </h2>

<p style="font-style: italic; color: grey; font-size: 0.9em;">Session Chair: Jennifer Firth</p>

<div class="card  m-3">
<div class="card-body">

<table>
<tbody>

<tr><td valign="top" style="margin-right: 5em; padding: 5px;"></td>
<td valign="top" style="margin-right: 5em; padding: 5px;">11:10-12:10</td><td style="padding: 5px;"><b> The CPC3 Poster Session
  </b> </td>
</tr>

<tr><td valign="top" style="margin-right: 5em; padding: 5px;"></td>
<td valign="top" style="margin-right: 5em; padding: 5px;">Poster 1</td><td style="padding: 5px;"><b> Non-Intrusive Multi-Branch Speech Intelligibility Prediction using Multi-Stage Training  </b><br /> <span class="author"> Ryandhimas E. Zezario<sup>1</sup>, Szu-Wei Fu<sup>2</sup>, Dyah A.M.G. Wisnu<sup>1,3</sup>, Hsin-Min Wang<sup>1</sup>, Yu Tsao<sup>1</sup></span> <i>( <sup>1</sup>Academia Sinica; <sup>2</sup>NVIDIA; <sup>3</sup>National Chengchi University)</i></td>
</tr>

<tr><td valign="top" style="margin-right: 5em; padding: 5px;"></td>
<td valign="top" style="margin-right: 5em; padding: 5px;">Poster 2</td><td style="padding: 5px;"><b> Domain-Adapted Automatic Speech Recognition with Deep Neural Networks for Enhanced Speech Intelligibility Prediction  </b><br /> <span class="author">Haeseung Jeon<sup>1</sup>, Jiwoo Hong<sup>1</sup>, Saeyeon Hong<sup>1</sup>, Hosung Kang<sup>1</sup>, Bona Kim<sup>1</sup>, Se Eun Oh<sup>1</sup>,
and Noori Kim<sup>2</sup></span> <i>( <sup>1</sup>Ewha Womans University, South Korea; <sup>2</sup>Purdue University, US )</i></td>
</tr>

<tr><td valign="top" style="margin-right: 5em; padding: 5px;"></td>
<td valign="top" style="margin-right: 5em; padding: 5px;">Poster 3</td><td style="padding: 5px;"><b> Predicting Intelligibility for Hearing-Impaired Listeners via Explicit Scores and Pre-trained Feature  </b><br /> <span class="author">Hanglei Zhang, Yanchen Li, Xiang Hao, Yufei Zhang, Jibin Wu, Kay Chen Tan</span> <i>(Hong Kong Polytechnic University, China)</i></td>
</tr>

<tr><td valign="top" style="margin-right: 5em; padding: 5px;"></td>
<td valign="top" style="margin-right: 5em; padding: 5px;">Poster 4</td><td style="padding: 5px;"><b> Non-Intrusive Speech Intelligibility Prediction Using Whisper ASR and Wavelet Scattering Embeddings for Hearing-Impaired Individuals  </b><br /> <span class="author">Rantu Buragohain<sup>1</sup>, Jejariya Ajaybhai<sup>1</sup>, Aashish Kumar Singh<sup>1</sup>, Karan Nathwani<sup>1</sup>, Sunil Kumar
Kopparapu<sup>2</sup></span> <i>( <sup>1</sup>Indian Institute of Technology Jammu, India; <sup>2</sup>Tata Consultancy Services Limited Mumbai, India)</i></td>
</tr>

<tr><td valign="top" style="margin-right: 5em; padding: 5px;"></td>
<td valign="top" style="margin-right: 5em; padding: 5px;">Poster 5</td><td style="padding: 5px;"><b> Integrating Linguistic and Acoustic Cues for Machine Learning-Based Speech Intelligibility Prediction in Hearing Impairment  </b><br /> <span class="author">Candy Olivia Mawalim, Xiajie Zhou, Huy Quoc Nguyen, Masashi Unoki</span> <i>(JAIST, Japan)</i></td>
</tr>

<tr><td valign="top" style="margin-right: 5em; padding: 5px;"></td>
<td valign="top" style="margin-right: 5em; padding: 5px;">Poster 6</td><td style="padding: 5px;"><b> OSQA-SI: A Lightweight Non-Intrusive Analysis Model for Speech Intelligibility Prediction  </b><br /> <span class="author">Hsing-Ting, Chen, Po-Hsun Sung</span> <i>(Merry Electronics Co.)</i></td>
</tr>

<tr><td valign="top" style="margin-right: 5em; padding: 5px;"></td>
<td valign="top" style="margin-right: 5em; padding: 5px;">Poster 7</td><td style="padding: 5px;"><b> Non-intrusive Speech Intelligibility Prediction Model for Hearing Aids using Multi-domain Fused Features  </b><br /> <span class="author">Guojian Lin, Fei Chen</span> <i>(Southern University of Science and Technology, Shenzhen, China)</i></td>
</tr>

<tr><td valign="top" style="margin-right: 5em; padding: 5px;"></td>
<td valign="top" style="margin-right: 5em; padding: 5px;">Poster 8</td><td style="padding: 5px;"><b> Word-level intelligibility model for the third Clarity Prediction Challenge  </b><br /> <span class="author">Mark Huckvale</span> <i>(University College London, UK)</i></td>
</tr>

<tr><td valign="top" style="margin-right: 5em; padding: 5px;"></td>
<td valign="top" style="margin-right: 5em; padding: 5px;">Poster 9</td><td style="padding: 5px;"><b> A Chorus of Whispers: Modeling Speech Intelligibility via Heterogeneous Whisper Decomposition  </b><br /> <span class="author">Longbin Jin, Donghun Min, Eun Yi Kim</span> <i>(Konkuk University, South Korea)</i></td>
</tr>

<tr><td valign="top" style="margin-right: 5em; padding: 5px;"></td>
<td valign="top" style="margin-right: 5em; padding: 5px;">Poster 10</td><td style="padding: 5px;"><b> Speech intelligibility prediction based on syllable tokenizer  </b><br /> <span class="author">Szymon Drgas</span> <i>(Poznan University of Technology, Poland)</i></td>
</tr>

<tr><td valign="top" style="margin-right: 5em; padding: 5px;"></td>
<td valign="top" style="margin-right: 5em; padding: 5px;">Poster 11</td><td style="padding: 5px;"><b> Modeling normal and impaired hearing with deep neural networks optimized for ecological tasks </b><br /> <span class="author">Mark R. Saddler<sup>1</sup>, Torsten Dau<sup>1</sup>, Josh H. McDermott<sup>2</sup></span> <i>( <sup>1</sup>DTU, Denmark; <sup>2</sup>MIT, US)</i></td>
</tr>

</tbody>
</table>

</div>
</div>

<h2 id="oral1"> Oral Session </h2>

<p style="font-style: italic; color: grey; font-size: 0.9em;">Session Chair: Jon Barker</p>

<div class="card  m-3">
<div class="card-body">

<table>
<tbody>

<tr><td valign="top" style="margin-right: 5em; padding: 5px;"></td>
<td valign="top" style="margin-right: 5em; padding: 5px;">12:10-12:30</td><td style="padding: 5px;"><b> Lightweight Speech Intelligibility Prediction with Spectro-Temporal Modulation for Hearing-Impaired Listeners
  </b><br /> <span class="author">Xiajie Zhou, Candy Olivia Mawalim, Huy Quoc Nguyen, Masashi Unoki</span> <i>(JAIST, Japan)</i> </td>
</tr>

<tr><td valign="top" style="margin-right: 5em; padding: 5px;"></td>
<td valign="top" style="margin-right: 5em; padding: 5px;">12:30-12:50</td><td style="padding: 5px;"><b> Intrusive Intelligibility Prediction with ASR Encoders
  </b><br />  <span class="author">Hanlin Yu<sup>1</sup>, Haoshuai Zhou<sup>2</sup>, Linkai Li<sup>2,3</sup>, Boxuan Cao<sup>2</sup>, Changgeng Mo<sup>2</sup>, Shan Xiang Wang<sup>3</sup></span> <i>( <sup>1</sup>University of British Columbia,Canada; <sup>2</sup>Orka Labs Inc., China; <sup>3</sup>Stanford University, US)</i> </td>
</tr>

<tr><td valign="top" style="margin-right: 5em; padding: 5px;"></td>
<td valign="top" style="margin-right: 5em; padding: 5px;">12:50-13:10</td><td style="padding: 5px;"><b> Towards individualized models of hearing-impaired speech perception
  </b><br />  <span class="author">Mark R. Saddler<sup>1</sup>, Torsten Dau<sup>1</sup>, Josh H. McDermott<sup>2</sup></span> <i>( <sup>1</sup>DTU, Denmark; <sup>2</sup>MIT, US)</i> </td>
</tr>

</tbody>
</table>

</div>
</div>
<!---------------------------------------------------->

<h1 id="discussion">ISAAR/Clarity Discussion Session</h1>

<p style="font-style: italic; color: grey; font-size: 0.9em;">Session Chair: Simone Graetzer</p>

There will be a one hour discussion session that will link up the International Symposium on Auditory and Audiological Research (<a href="https://isaar.eu/">ISAAR-2025</a>) and the Clarity workshop. The discussion will bring speech and hearing researchers together to discuss the future priorities for hearing device machine learning challenges. What are the needs of hearing device users? What opportunities does machine learning provide for addressing these needs?

<div class="card  m-3">
<div class="card-body">

<table>
<tbody>

<tr><td valign="top" style="margin-right: 5em; padding: 5px;"></td>
<td valign="top" style="margin-right: 5em; padding: 5px;">14:00-14:10</td><td style="padding: 5px;"><b> Opening Remarks: ISAAR Workshop
  </b> </td> </tr>

<tr><td valign="top" style="margin-right: 5em; padding: 5px;"></td>
<td valign="top" style="margin-right: 5em; padding: 5px;">14:10-14:20</td><td style="padding: 5px;"><b> Opening Remarks: Clarity Workshop
  </b> </td> </tr>

<tr><td valign="top" style="margin-right: 5em; padding: 5px;"></td>
<td valign="top" style="margin-right: 5em; padding: 5px;">14:20-14:50</td><td style="padding: 5px;"><b> Discussion
  </b> </td> </tr>

<tr><td valign="top" style="margin-right: 5em; padding: 5px;"></td>
<td valign="top" style="margin-right: 5em; padding: 5px;">14:50-15:00</td><td style="padding: 5px;"><b> Summary/Conclusion
  </b> </td> </tr>

</tbody>
</table>

</div>

</div>

Further session details to appear.

<!---------------------------------------------------->

<h1 id="oral2">Hearing Aid Speech Enhancement + Other Topics</h1>

<p style="font-style: italic; color: grey; font-size: 0.9em;">Session Chair: Jesper Jensen</p>

The session will consist of oral presentations each of up to 20 minutes with 5 minutes for Q&A.

<div class="card  m-3">
<div class="card-body">

<table>
<tbody>

<tr><td valign="top" style="margin-right: 5em; padding: 5px;"></td>
<td valign="top" style="margin-right: 5em; padding: 5px;">15:20-15:45</td><td style="padding: 5px;"><b> The Dawn of Psychoacoustic Reverse Correlation: A Data-Driven Methodology for Determining Fine Grained Perceptual Cues of Speech Clarity
  </b><br /> <span class="author">Paige Tutt√∂si<sup>1</sup>, Henny Yeung<sup>1</sup>, Yue Wang<sup>1</sup>, Jean-Julien Aucouturier<sup>2</sup>, Angelica Lim<sup>1</sup>
</span> ( <i><sup>1</sup>Simon Fraser University, US; <sup>2</sup>Institut FEMTO-ST, France</i>)</td>
</tr>

<tr><td valign="top" style="margin-right: 5em; padding: 5px;"></td>
<td valign="top" style="margin-right: 5em; padding: 5px;">15:45-16:10</td><td style="padding: 5px;"><b> TF-MLPNet: Tiny Real-Time Neural Speech Separation  </b><br /> <span class="author">Malek Itani, Tuochao Chen, Shyam Gollakota </span> (<i>University of Washington, US</i>)</td>
</tr>

<tr><td valign="top" style="margin-right: 5em; padding: 5px;"></td>
<td valign="top" style="margin-right: 5em; padding: 5px;">16:10-16:35</td><td style="padding: 5px;"><b> Controllable joint noise reduction and hearing loss compensation using a differentiable auditory model  </b><br /> <span class="author">Philippe Gonzalez, Tobias May, Torstan Dau</span> <i>(Technical University of Denmark)</i></td>
</tr>

<tr><td valign="top" style="margin-right: 5em; padding: 5px;"></td>
<td valign="top" style="margin-right: 5em; padding: 5px;">16:35-17:00</td><td style="padding: 5px;"><b> Say Who You Want to Hear: Leveraging TTS Style Embeddings for Text-Guided Speech Extraction  </b><br /> <span class="author">Akam Rahimil, Triantafyllos Afouras, Andrew Zisserman</span> <i>(University of Oxford, UK)</i></td>
</tr>

</tbody>
</table>

</div>
</div>

<!---------------------------------------------------->

<h1 id="next">Next Steps</h1>

Presenting upcoming/ongoing machine challenges relevant to the community.

<div class="card  m-3">
<div class="card-body">

<table>
<tbody>

<!--<div style="margin-bottom: 1em;">Session chair: <i>TBC</i></div>-->

<tr><td valign="top" style="margin-right: 5em; padding: 5px;"></td>
<td valign="top" style="margin-right: 5em; padding: 5px;">17:00-17:10</td><td style="padding: 5px;"><b> From spoken speech to sung speech - Cadenza Lyric Challenge
  </b><br /> <span class="author"> Micheal Akeroyd<sup>2</sup>, Scott Bannister<sup>1</sup>, Jon Barker<sup>4</sup>, Trevor Cox<sup>3</sup>, Bruno Fazenda<sup>3</sup>, Simone Graetzer<sup>3</sup>, Alinka Greasley<sup>1</sup>, Gerardo Roa<sup>4</sup>, Rebecca Vos<sup>3</sup>, William Whitmer<sup>2</sup> </span> ( <i><sup>1</sup>University of Leeds, UK; <sup>2</sup>University of Nottingham, UK; <sup>3</sup>University of Salford, UK; <sup>4</sup>University of Sheffield, UK</i>)</td>
</tr>

<tr><td valign="top" style="margin-right: 5em; padding: 5px;"></td>
<td valign="top" style="margin-right: 5em; padding: 5px;">17:10-17:20</td><td style="padding: 5px;"><b> Processing conversations in noise - CHiME-9 ECHI Challenge
  </b><br /> <span class="author"> Jon Barker<sup>1</sup>, Stefan Goetze<sup>1</sup>, Robert Sutherland<sup>1</sup>, Marko Lugger<sup>2</sup>, Thomas Kuebert<sup>2</sup>, Stefan Petrausch<sup>2</sup>, Juan Azcarreta Ortiz<sup>3</sup>, Buye Xu<sup>3</sup> </span> ( <i> <sup>1</sup>University of Sheffield, UK; <sup>2</sup>WSAudiology, Germany; <sup>3</sup>Meta Reality Labs Research, UK</i>)</td>
</tr>

</tbody>
</table>

</div>
</div>
